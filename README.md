# DPA-Chicago-VLIN ‚úíÔ∏è

Repositorio para la clase de Arquitectura de Producto de Datos, primavera 2021-ITAM

| Nombre | usuario git |
|-------|-----------------|
|Arenas Morales Nayeli | arenitss |
|Hern√°ndez Mart√≠nez Luz Aurora | LuzVerde23 |
|Santiago Castillejos Ita Andehui | sancas96 |
|S√°nchez Guti√©rrez Vianney | visagu55 |

# Summary de los datos: üìã

Datos:
- [**Chicago Food Inspections**](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5)

Al 15 de enero de 2021 a las 7:39 p.m.
  - N√∫mero de registros: 215,067
  - N√∫mero de columnas: 17
  - Qu√© variables son, qu√© informaci√≥n tiene:

    - **Inspection ID**: Identificador consecutivo. Tipo num√©rico.
    - **DBA Name**: Acr√≥nimo de 'Doing business as', que es el nombre legal del establecimiento. Tipo texto.
    - **AKA Name**: Acr√≥nimo de 'Also known as' el nombre por el que es conocido el establecimiento. Tipo texto.
    - **License #**: N√∫mero de licencia asignada por el 'Department of Business Affairs and Consumer Protection'. Tipo num√©rico.
    - **Facility Type**: Tipo de servicio seg√∫n su descripci√≥n: bakery, banquet hall, candy store, caterer, coffee shop, day care center (for ages less than 2), day care center (for ages 2 ‚Äì 6), day care center (combo, for ages less than 2 and 2 ‚Äì 6 combined), gas station, Golden Diner, grocery store, hospital, long term care center(nursing home), liquor store, mobile food dispenser, restaurant, paleteria, school, shelter, tavern, social club, wholesaler, or Wrigley Field Rooftop. Tipo texto.
    - **Risk**: Cada establecimiento se categoriza seg√∫n el tipo de riesgo a la salud, esto es 1 (el m√°s alto) a 3 (el m√°s bajo). Tipo texto.
    - **Address**: Direcci√≥n para facilitar su ubicaci√≥n. Tipo texto.
    - **City**: Ciudad. Tipo texto.
    - **State**: Estado. Tipo texto.
    - **Zip**: C√≥digo postal. Tipo num√©rico.
    - **Inspection Date**: Describe la fecha en que la inspecci√≥n ocurri√≥, un establecimiento puede tener m√∫ltiples inspecciones. Tipo de fecha y hora.
    - **Inspection Type**: Las inspecciones se pueden describir como sigue:
        * canvass: el tipo m√°s com√∫n de inspecci√≥n que se ejecuta con una frecuencia relativa al riesgo del establecimiento.
        * consultation: cuando la inspecci√≥n se realiza a petici√≥n del due√±o previo a la apertura del establecimiento.
        * complaint: cuando la inspecci√≥n se realiza en respuesta a una queja en contra del establecimiento.
        * license: cuando la inspecci√≥n se realiza como un requerimiento para que el establecimiento pueda recibir su licencia para operar.
        * suspect food poisoning: inspecci√≥n que se realiza en respuesta a una o m√°s personas que indican haberse enfermado como resultado de haber comido en el establecimiento.
        * task-force inspection: cuando la inspecci√≥n de un bar o taberna se ejecuta. Tipo texto.

      La re-inspecci√≥n puede ocurrir para todos los tipos de inspecciones y se nombrar√≠an de la misma manera.
    - **Results**: Muestra el resultado de la inspecci√≥n bajo las siguientes categor√≠as: puede aprobarse, aprobarse con condiciones o fallar. Se encontr√≥ que "pasar" no ten√≠a violaciones cr√≠ticas o graves (violaci√≥n n√∫mero 1-14 y 15-29, respectivamente).Las categorias pueden ser: 'pass', 'pass with conditions' y 'fail'.  Tipo texto.
    - **Violations**: Un establecimiento puede recibir uno o m√°s de 45 (1-44 y 70) infracciones distintas a la norma. Adem√°s se enuncia el requisito que el establecimiento debe cumplir para NO recibir una infracci√≥n, seguido de una descripci√≥n espec√≠fica de los hallazgos que causaron la violaci√≥n. Tipo texto.
    - **Latitude**: Latitud del establecimiento. Tipo num√©rico.
    - **Longitude**: longitud del establecimiento. Tipo num√©rico.
    - **Location**: la latitud y longitud del establecimiento. Tipo localizaci√≥n.

  - La pregunta anal√≠tica que queremos resolver es: ¬øEl establecimiento pasar√° o no la inspecci√≥n?
  - Frecuencia de actualizaci√≥n de los datos: Diaria, aunque para efectos del proyecto ser√° de manera semanal.


# Reproducibilidad y requerimientos. ‚öôÔ∏è

**Importante** Este proyecto debe ser ejecutado desde el ambiente de trabajo seleccionado, ejecutando `pyenv activate <<tu_ambiente>>`.

Para la infraestructura de este proyecto ocupamos la siguiente arquitectura:

| **Basti√≥n** | **EC2 de procesamiento** | **RDS** |
|-------|-------| -------|
| Ubuntu Server 18.04| Ubuntu Server 18.04| PostgreSQL 12.5-R1 |
| 64 bits (x86) | 64 bits (x86) |
| t2.micro | t2.medium | db.t2.micro  |
| Volumen 20 GiB | Volumen 80 GiB |

Las cuales se iran ocupando a lo largo de esta lectura.

Para este proyecto utilizamos la versio≈Ñ **Python 3.7.4**
1. Para la reproducibilidad del an√°lisis exploratorio de datos: en la carpeta data, colocar el archivo `Food_Inspections.csv` que est√° disponible en este [**Drive**](https://drive.google.com/file/d/1Pyobds5_o_4wKHbZQTsmzfVd-NszjEQM/view?usp=sharing)
2. Para la reproducibilidad de los _tasks_ se creo  la infraestructura en AWS ya mencionada, a la cual tendremos acceso con :

#### Basti√≥n üìñ

  Para tener acceso a cualquiera de estos (Basti√≥n, EC2 y RDS) se requiere que el administrador les haya dado acceso a los mismos y tener un usuario asignado. Con lo anterior ya cumplido, hay que correr en su terminal lo siguiente:

```
    ssh -o ServerAliveInterval=60 -i ~/.ssh/<<llave_privada>> <<tu_usuario>>@ip_del_ec2
```

#### EC2 üîß

  Aqu√≠ es donde se encuentra toda la estructura de este repositorio.

```
    ssh -o ServerAliveInterval=60 -i ~/.ssh/<<llave_privada>> <<tu_usuario>>@ip_del_ec2
```
#### rds üì¶

  _Framework_ con los metadatas de almacenamiento y procesamiento

```
    psql -U chicago_food -h chicago-food-2021.ctd292l1zdjq.us-west-2.rds.amazonaws.com -d chicago_food
```

3. En el ambiente virtual hay que instalar las librer√≠as del archivo requirements.txt que se encuentra dentro de este repositorio: `pip install -r requirements.txt`
4. En la terminal debemos estar ubicados en la carpeta de este repositorio y ejecutar un `export PYTHONPATH=$PWD`

5. Para poder tener el mismo esqueleto de la base de datos en postgress se debe crear un usuario y despu√©s crear la base de datos y darle los permisos correspondientes:
```
  sudo -u postgres createuser --login --pwprompt chicago_food
  create database chicago_food;
  sudo -u postgres createdb --owner=chicago_user chicago_food
```
Despu√©s de este paso es necesario crear los esquemas como se sugiere en el `script`  que est√° en la ruta `sql`.

6. La carpeta `conf/local/` debe contener las credenciales para la conexi√≥n tanto al _bucket_ en aws (s3), el _token_ para obtener la informaci√≥n de la base de datos a la que nos estamos conectando (food_inspections) y las credenciales para la conexi√≥n a la base de datos relacional donde se guardar√° nuestra informaci√≥n.

+ Las llaves de `s3` son para interactuar de manera m√°s sencilla con el servicio de almacenamiento de archivos de `aws`.

+ El apartado de `food_inspections` debe contener la llave `api_token` que es el token generado desde [**aqu√≠**](https://data.cityofchicago.org/login?return_to=%2Fprofile%2Fedit%2Fdeveloper_settings) que funcionar√° para hacer la ingesti√≥n de la API. Para m√°s informaci√≥n se puede consultar [**aqu√≠**](https://dev.socrata.com/foundry/data.cityofchicago.org/4ijn-s7e5).

+ Asimismo, la carpeta debe contener las credenciales para ingresar a la base de datos (RDS).

Este archivo deber√° ser llamado `credentials.yaml` con el siguiente esqueleto.

```
s3:
  aws_access_key_id : "xxxxxx"
  aws_secret_access_key : "xxxxxx"
food_inspections:
  api_token: "xxxxxxx"
chicago_database:
  user: "chicago_food"
  password: "xxxxxxx"
  database: "chicago_food"
  host: "chicago-food-2021.ctd292l1zdjq.us-west-2.rds.amazonaws.com"
  port: "5432"
```

<img width="1020" alt="imagen" src="https://github.com/sancas96/DPA-Chicago-VLIN/blob/main/images/ec2_security_groups.png">

Tomado de [data-product-architecture](https://github.com/ITAM-DS/data-product-architecture)

-----

# An√°lisis Exploratorio ‚å®Ô∏è
El notebook `Chicago_food_inspections.ipynb` con el an√°lisis exploratorio se encuentra en la carpeta `notebooks/eda/`. Para este an√°lisis se uso como informaci√≥n de corte el archivo .csv mencionado en el punto 1 del inciso anterior.

# Luigi üõ†Ô∏è
## Ingesta y almacenamiento

1. Se asume que dentro de _aws_ se tenga levantado un bucket llamado `data-product-architecture-equipo8` con la siguiente estructura:
```
    ‚îú‚îÄ‚îÄ data-product-architecture-equipo8
    ‚îÇ   ‚îú‚îÄ‚îÄ ingesta    
```
2. Para poder visualizar los ejercicios en EC2 de procesamiento se realiza un doble espejo que va de:
```
        EC2 de procesamiento -> Basti√≥n -> tu computadora
```
+ En EC2 de procesamiento se ejecuta `luigid`.
+ En basti√≥n realizas el primer _portforwdaring_:
```
ssh -i ~/.ssh/<<llave_privada>> -NL localhost:4444:localhost:8082 <<usuario>>@ip_del_ec2
```
+ En tu computadora realizas el segundo _portforwdaring_:
```
ssh -i ~/.ssh/<<llave_privada>>-NL localhost:4444:localhost:4444 <<usuario>>@ip_del_ec2
```
 En el navegador entras a [http://localhost:4444](http://localhost:4444)

3. Luigi

Para la ingesta, almacenamiento, limpieza, ingenier√≠a de caracter√≠sticas, entrenamiento y selecci√≥n del modelo ocuparemos como orquestador a [Luigi](https://luigi.readthedocs.io/en/stable/index.html). Para cada una de estas tareas los parametros necesarios pueden ser los siguientes:

- **tipo_ingesta**: historica o consecutiva.
- **fecha**: Fecha en la que se est√° haciendo la ingesta con respecto a inspection date.
- **bucket**: nombre de tu bucket en `aws`.
- **tamanio**: tama√±o del archivo almacenado.
- **tipo-prueba**: infinito o size

La estructura desarrollada es la siguiente:

  Ingesta inicial y metadata: Con las credenciales que se dieron de alta para conectarnos a la API de _data.cityofchicago.org_, descargamos la base de datos disponible hasta la fecha. Este archivo se guardara con el nombre `historica-{fecha}.pkl`
```
PYTHONPATH="." luigi --module src.pipeline.metadata_almacenamiento metadata_almacenar --tipo-ingesta historica --fecha 2021-03-29T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 100
```    
  Ingesta consecutiva: Es la descarga de los datos posteriores a la ingesta inicial hasta la fecha solicitada. Este archivo se guardara con el nombre `consecutiva-{fecha}.pkl`
```
PYTHONPATH="." luigi --module src.pipeline.metadata_almacenamiento metadata_almacenar --tipo-ingesta consecutiva --fecha 2021-04-05T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 100
```
Cada uno de estos ejemplos almacena tambi√©n la _metadata_ de cada uno de los procesos, esto es en la tabla de _rds_ `metadata_ingesta` y `metadata_almacenar`.

# Limpieza de datos
Con la base de datos obtenida en las tareas de ingesti√≥n y almacenamiento, hacemos un proceso de limpieza donde:

  - Se eliminan los datos nulos de las variables `inspection_date`, `license_`, `latitude`, `longitude`,
  - Se eligen solo los establecimientos que est√°n en operaci√≥n,
  - Se eliminan los duplicados,
  - Se sustituyen los datos nulos restantes con cero.

Metadata de limpieza de datos: Guardamos la metadata generada por el proceso de limpieza. Este es un ejemplo de c√≥mo correrlo
```
PYTHONPATH="." luigi --module src.pipeline.metadata_limpieza metadata_limpiar --tipo-ingesta consecutiva --fecha 2021-04-12T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 100 --tipo-prueba infinito
```
Este proceso genera las tablas `data.limpieza`, `data.metadata_limpieza` que contiene la tabla con esta limpieza y la _metadata_ de la misma, respectivamente.

# Ingenier√≠a de caracter√≠sticas
Con los datos limpios, corremos el proceso de ingenier√≠a de caracter√≠sticas en donde:
  - Convertimos la variable de infracciones en columnas de tipo dummy,
  - Aplicamos label encoding (convertir a categor√≠as num√©ricas variables categ√≥ricas de tipo string),
  - Eliminamos las variables que no aportan informaci√≥n relevante al modelo.

Metadata de ingenier√≠a de caracter√≠sticas: Guardamos la metadata generada por el proceso de ingenier√≠a de caracter√≠sticas.
```
PYTHONPATH="." luigi --module src.pipeline.metadata_ingenieria_caract metadata_ingenieria --tipo-ingesta consecutiva --fecha 2021-04-15T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 100 --tipo-prueba infinito
```
Este proceso genera las tablas `data.ingenieria`, `data.metadata_ingenieria` que contiene la tabla con esta ingenier√≠a de caracter√≠sticas y la _metadata_ de la misma, respectivamente.

# Entrenamiento
Con el dataset listo, se divide en entrenamiento y prueba, con porcentajes del 75% y 25% respectivamente. Se corren los siguientes tres modelos de clasificaci√≥n haciendo uso de la librer√≠a _scikit learn_:
  - XGboost,
  - KNN,
  - Logistic Regression.

De estos tres calculamos el _accuracy_ y guardamos cada modelo y sus par√°metros en archivos pkl.

Metadata de entrenamiento de modelos: Guardamos la metadata generada por el proceso de entrenamiento.
```
PYTHONPATH="." luigi --module src.pipeline.metadata_entrenamiento metadata_entrenar --tipo-ingesta consecutiva --fecha 2021-04-23T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 100 --tipo-prueba infinito

PYTHONPATH="." luigi --module src.pipeline.test_entrenamiento test_entrenar --tipo-ingesta consecutiva --fecha 2021-04-23T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 1000000000 --tipo-prueba infinito
```

# Selecci√≥n de modelo
En esta parte se pretende tomar el modelo con el mejor _accuracy_, as√≠ que elegimos el m√°ximo _accuracy_ de los 3 modelos de entrenamiento.

Metadata de selecci√≥n del modelo: Guardamos la metadata generada por el proceso de selecci√≥n del modelo.
```
PYTHONPATH="." luigi --module src.pipeline.metadata_seleccion metadata_seleccionar --tipo-ingesta consecutiva --fecha 2021-04-23T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 100 --tipo-prueba infinito

PYTHONPATH="." luigi --module src.pipeline.metadata_seleccion metadata_seleccionar --tipo-ingesta consecutiva --fecha 2021-04-23T00:00:00.00 --bucket data-product-architecture-equipo8 --tamanio 100 --tipo-prueba shape
```

Si las sentencias anteriores se corren en el orden indicado, podremos ver un _DAG_ de Luigi similar a este:

<img width="1020" alt="imagen" src="https://github.com/sancas96/DPA-Chicago-VLIN/blob/main/images/luigi_checkpoint4.png">

# Nota:
Este producto de datos contin√∫a en desarrollo, por lo que a√∫n faltan algunas mejoras,recomendaciones o mejores pr√°cticas que se estar√°n atendiendo:
- Las sentencias que se corren de luigi idealmente no deber√≠an contener en la fecha el formato de tiempo.
- Igualmente en la sentencia de luigi lo ideal ser√≠a no introducir un par√°metro para el nombre del _bucket_ e incluirlo como parte de una constante en el archivo `constants.py`.


**Nota:** El usuario lmillan ya fue asignado con la llave correspondiente.

---

**Figura 1**. Estructura b√°sica del proyecto.

```  
‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
‚îú‚îÄ‚îÄ conf
‚îÇ   ‚îú‚îÄ‚îÄ base           <- Space for shared configurations like parameters
‚îÇ   ‚îî‚îÄ‚îÄ local          <- Space for local configurations, usually credentials
‚îú‚îÄ‚îÄ data               <- Space for data
‚îú‚îÄ‚îÄ docs               <- Space for Sphinx documentation
‚îú‚îÄ‚îÄ images             <- Space for images
‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks.
‚îÇ
‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
‚îÇ
‚îú‚îÄ‚îÄ results            <- Intermediate analysis as HTML, PDF, LaTeX, etc.
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file
‚îÇ
‚îú‚îÄ‚îÄ .gitignore         <- Avoids uploading data, credentials, outputs, system files etc
‚îÇ
‚îú‚îÄ‚îÄ infrastructure
‚îú‚îÄ‚îÄ sql
‚îú‚îÄ‚îÄ setup.py
‚îî‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ
    ‚îú‚îÄ‚îÄ utils          <- Functions used across the project
    ‚îÇ
    ‚îÇ
    ‚îú‚îÄ‚îÄ etl            <- Scripts to transform data from raw to intermediate
    ‚îÇ
    ‚îÇ
    ‚îî‚îÄ‚îÄ pipeline       <- Scripts to data ingestion
```
